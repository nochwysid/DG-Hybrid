{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc_nb'></a> \n",
    "  \n",
    "[Model Definition](#model_def)  \n",
    "[Hybrid Loss](#hybrid)  \n",
    "[Training](#train)  \n",
    "[Testing](#test)  \n",
    "[Visualization](#vis)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, json, jsonpickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tracking = np.zeros(shape=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save_load'></a> \n",
    "\n",
    "###### [Back to TOC](#toc_nb)  [Next ](#data_load) \n",
    "\n",
    "### Model Saving and loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for converting state_dict to JSON format\n",
    "def saveModel(model):\n",
    "    json_str = jsonpickle.encode(model.state_dict())\n",
    "    # Save best model for later use\n",
    "    out_file = open(\".json\", \"w\")\n",
    "    json.dump(json_str, out_file, indent = 6)\n",
    "    out_file.close()\n",
    "\n",
    "# Load saved model\n",
    "def loadModel():\n",
    "    in_file = open(\".json\", \"r\")\n",
    "    input = json.load(in_file)\n",
    "    thawed = jsonpickle.decode(input)\n",
    "    in_file.close()\n",
    "    return thawed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#a = [0 for i in range(5)]\n",
    "import numpy as np\n",
    "a = np.zeros(shape=5)\n",
    "print(a)\n",
    "a[4] = 1\n",
    "print(a)\n",
    "for i in range(1,5):\n",
    "    a[i-1] = a[i]\n",
    "a[4] = 0\n",
    "print(a)\n",
    "for i in range(1,5):\n",
    "    a[i-1] = a[i]\n",
    "a[4] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD: 0.4000000000000001, Var: 0.16000000000000006\n"
     ]
    }
   ],
   "source": [
    "print(f\"SD: {a.std()}, Var: {a.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_load'></a> \n",
    "\n",
    "###### [Back to TOC](#toc_nb) [Previous ](#save_load) [Next ](#model_def) \n",
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1000, 784]) torch.Size([1000])\n",
      "<class 'torch.Tensor'> torch.Size([100, 784]) torch.Size([100])\n",
      "cuda:0\n",
      "Cell 1 Done\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    ds = pd.read_csv(path)\n",
    "    size = ds.shape[0]%64\n",
    "    features = torch.tensor(ds.iloc[:, 1:].values, \n",
    "                            dtype=torch.float32)\n",
    "    #features = torch.tensor(ds.iloc[:, 1:].values, \n",
    "    #                        dtype=torch.float32).reshape(ds.shape[0],28,28)\n",
    "    labels = torch.tensor(ds.iloc[:, 0].values, dtype=torch.float32)\n",
    "    print(type(features), features.shape,labels.shape)\n",
    "\n",
    "    return features, labels\n",
    "       \n",
    "#training_data = load_data(\"~/path/to/datasets\")\n",
    "#test_data = load_data(\"~/path/to/datasets\")\n",
    "train_feat, train_lbl = load_data(\"~/path/to/datasets\")\n",
    "train_feat = train_feat.to('cuda')\n",
    "train_lbl = train_lbl.to('cuda')\n",
    "test_feat, test_lbl = load_data(\"~/path/to/datasets\")\n",
    "test_feat = test_feat.to('cuda')\n",
    "test_lbl = test_lbl.to('cuda')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "print(test_lbl.device)\n",
    "print(f\"Cell 1 Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_def'></a>   \n",
    "\n",
    "###### [Back to TOC](#toc_nb)  [Previous ](#data_load) [Next ](#evol)  \n",
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the dropout? A separate forward func for dis and gen, \n",
    "# later, one func for both last layer problem and hybrid loss \n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers, activations):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) #input layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) #hidden layer\n",
    "        self.Dis = nn.Linear(hidden_size, output_size)\n",
    "        self.Gen = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # This allows making a variable size newtwork\n",
    "\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if type(layer) == nn.Linear:\n",
    "                self.layer = layer.to('cuda')\n",
    "                self.layers.append(self.layer)\n",
    "        self.activations = activations \n",
    "        self.layeractivationpairs = zip(self.layers,self.activations)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x) #input layer\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        #print(\"forward is being called\")\n",
    "        dX = self.Dis(x)\n",
    "        gX = self.Gen(x)\n",
    "        #x = sm(x)\n",
    "        return dX, gX\n",
    "        #return x \n",
    "    \n",
    "    def altforward(self,x):\n",
    "        for layer,activation in self.layers,self.activations:\n",
    "            print(layer,activation)\n",
    "            print(layer.state_dict,activation.state_dict)\n",
    "            x = layer(x)\n",
    "            x = activation(x)\n",
    "        return x\n",
    "    \n",
    "    def copy(self, model):\n",
    "        self.load_state_dict(model.state_dict())\n",
    "    \n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "\n",
    "# Create an instance of the neural network\n",
    "input_size = 784\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "layers=[nn.Linear(784, 20),nn.Linear(20, 10)]\n",
    "activations=[nn.ReLU(),nn.ReLU()]\n",
    "\n",
    "dg_model=[MyNN(input_size, hidden_size, output_size,layers, activations).to('cuda') for i in range(CENSUS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(tensor):\n",
    "    vx,idxx = torch.max(tensor,1,keepdim=True)\n",
    "    vy,idxy = torch.max(tensor,0,keepdim=True)\n",
    "    #print(f\"\\nvx: {vx}, vy: {vy}\\n\")\n",
    "    x=int(torch.argmax(idxx))\n",
    "    y=int(torch.argmax(idxy))\n",
    "   # print(f\"idxx shape: {idxx.shape}, idxy shape: {idxy.shape}\")\n",
    "    #x=int(idxx[x,0])\n",
    "    #y=int(idxy[0,y])\n",
    "    ix=int(idxx[x,0])\n",
    "    iy=int(idxy[0,y])\n",
    "    #print(f\"idxx: {idxx}, idxy: {idxy}, \\nx: {x}, y: {y}, \\n{tensor}\\n\")\n",
    "    #print(f\"value: {tensor[iy,ix]} at y: {iy}, x: {ix}\")\n",
    "    #print(float(tensor[x][y]) )\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hybrid'></a>   \n",
    "\n",
    "###### [Back to TOC](#toc_nb)  [Previous ](#evol) [Next ](#tt_def) \n",
    "### Hybrid Loss  \n",
    "This is where we give the details about the hybrid loss function. Benefits and limitations. Implementation details. Theory behind it.  \n",
    "\n",
    "#### Regularization  \n",
    "Due to the differences in the nature of the tasks, the weights may need regularization to not fall off the edge of reasonablenesses  \n",
    "#### Generative  \n",
    "To model the underlying data distribution. That is to say, the probablity of features, given the label. \n",
    "#### Discriminative  \n",
    "To model the likelihood of a sample coming from a distribution, given the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternative approach\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.kl_loss = nn.KLDivLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha  # Weight for KL\n",
    "        self.beta = beta    # Weight for Cross-Entropy\n",
    "\n",
    "    def forward(self, outputs, targets, aux_targets):\n",
    "        #Gloss = self.KLDivLoss()\n",
    "        # Compute KL Loss\n",
    "        kl = self.kl_loss(outputs, targets)\n",
    "        # Compute Cross-Entropy Loss\n",
    "        ce = self.ce_loss(outputs, aux_targets)\n",
    "        # Combine with weights\n",
    "        total_loss = self.alpha * kl + self.beta * ce\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet another alternative\n",
    "class GMMLogisticLoss(nn.Module):\n",
    "    def __init__(self, n_components=2, n_features=784):\n",
    "        super(GMMLogisticLoss, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # GMM parameters\n",
    "        self.means = nn.Parameter(torch.randn(n_components, n_features))\n",
    "        self.covs = nn.Parameter(torch.eye(n_features).repeat(n_components, 1, 1))\n",
    "        self.weights = nn.Parameter(torch.ones(n_components) / n_components)\n",
    "        \n",
    "        # Logistic loss\n",
    "        self.log_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def gmm_log_likelihood(self, x):\n",
    "        # Calculate GMM log likelihood for each component\n",
    "        log_probs = []\n",
    "        for k in range(self.n_components):\n",
    "            diff = x - self.means[k]\n",
    "            log_prob = -0.5 * (\n",
    "                torch.log(torch.det(self.covs[k])) +\n",
    "                torch.sum(torch.matmul(diff, torch.inverse(self.covs[k])) * diff, dim=1) +\n",
    "                self.n_features * np.log(2 * np.pi)\n",
    "            )\n",
    "            log_probs.append(log_prob + torch.log(self.weights[k]))\n",
    "        \n",
    "        return torch.logsumexp(torch.stack(log_probs), dim=0)\n",
    "\n",
    "    def forward(self, pred_dis, pred_gen, targets, features):\n",
    "        # Discriminative (Logistic) Loss\n",
    "        lr_loss = self.log_loss(pred_dis, targets)\n",
    "        \n",
    "        # Generative (GMM) Loss\n",
    "        gmm_loss = -torch.mean(self.gmm_log_likelihood(features))\n",
    "        \n",
    "        return lr_loss, gmm_loss\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, n_components=2, n_features=784):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight between GMM and LR losses\n",
    "        self.gmm_lr_loss = GMMLogisticLoss(n_components=n_components, n_features=n_features)\n",
    "        \n",
    "    def forward(self, pred_dis, pred_gen, targets, features):\n",
    "        lr_loss, gmm_loss = self.gmm_lr_loss(pred_dis, pred_gen, targets, features)\n",
    "        total_loss = self.alpha * lr_loss + (1 - self.alpha) * gmm_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Train(model,ins, outs, iter):\n",
    "    bestloss=1000.0\n",
    "    best=0\n",
    "   \n",
    "    criterion = HybridLoss(alpha=0.5, n_components=2, n_features=784)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    avgloss = 0\n",
    "    for i in range(ins.shape[0]):\n",
    "        optimizer.zero_grad()\n",
    "        #predicted = model(ins[i].reshape(1,784))\n",
    "        dis_out, gen_out = model(ins[i].reshape(1, 784))\n",
    "        #loss = max(outs)*Dloss(predicted, outs[i]) + (1 - max(outs))*Gloss(predicted, outs[i])\n",
    "        #loss = criterion(predicted, outs[i])\n",
    "        loss = criterion(dis_out, gen_out, outs[i], ins[i].reshape(1, 784))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(avgloss)\n",
    "        avgloss+=loss.item()\n",
    "    if avgloss < bestloss:\n",
    "        bestloss = avgloss\n",
    "        best = m\n",
    "        \n",
    "# Test the trained model\n",
    "#test_input = torch.randn(1, input_size).to('cuda')\n",
    "#with torch.no_grad():\n",
    "#    test_output = model(test_input)\n",
    "#print(\"Test output:\", test_output)\n",
    "\n",
    "def model_test(model,m_ins, m_outs):\n",
    "    print(m_ins.device)\n",
    "    score = 0\n",
    "\n",
    "    for i in range(m_ins.shape[0]):\n",
    "        p = model(m_ins[i].reshape(1,784))\n",
    "        gt = int(m_outs[i])\n",
    "        y = torch.zeros(1,10).to('cuda')\n",
    "        y[0,gt] = 1\n",
    "        tmp = y - p\n",
    "        #print(tmp, gt, p.shape, y.shape)\n",
    "        if torch.argmax(p)==torch.argmax(y):\n",
    "            score+=1\n",
    "    score = 100*score/m_ins.shape[0]\n",
    "    print(f\"Best result: {score}%\")\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>   \n",
    "\n",
    "###### [Back to TOC](#toc_nb)  [Previous ](#tt_def) [Next ](#test) \n",
    "Training  \n",
    "Maybe handle switching between losses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = model_test(dg_model,test_feat,test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model_test(dg_model,test_feat,test_lbl)\n",
    "\n",
    "EPOCHS = 8\n",
    "for i in range(EPOCHS):\n",
    "    Train(dg_model,train_feat,train_lbl)\n",
    "    if i%20==0:\n",
    "        print(f\"Training {100*i/(EPOCHS):.1f}% completed\")\n",
    "    \n",
    "top = model_test(dg_model,test_feat,test_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(dg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vis'></a>   \n",
    "\n",
    "###### [Back to TOC](#toc_nb)  [Previous ](#test) [Next ](#vis) \n",
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tmp_mdl.fc1.weight.cpu().detach()\n",
    "w2 = tmp_mdl.fc2.weight.cpu().detach()\n",
    "b1 = tmp_mdl.fc1.bias.cpu().detach()\n",
    "b2 = tmp_mdl.fc2.bias.cpu().detach()\n",
    "a = tmp_mdl.fc1.weight.cpu().detach()\n",
    "b = tmp_mdl.fc2.weight.cpu().detach()\n",
    "\n",
    "figure, WBplots = plt.subplots(2, 2)\n",
    "  \n",
    "# For Sine Function\n",
    "WBplots[0, 0].imshow(w1, cmap='hot', interpolation='nearest')\n",
    "WBplots[0, 0].set_title(\"1st layer weight\")\n",
    "  \n",
    "# For Cosine Function\n",
    "WBplots[0, 1].imshow(w2, cmap='hot', interpolation='nearest')\n",
    "WBplots[0, 1].set_title(\"2nd layer weight\")\n",
    "  \n",
    "# For Tangent Function\n",
    "WBplots[1, 0].imshow(b1, cmap='hot', interpolation='nearest')\n",
    "WBplots[1, 0].set_title(\"1st layer bias\")\n",
    "  \n",
    "# For Tanh Function\n",
    "WBplots[1, 1].imshow(b2, cmap='hot', interpolation='nearest')\n",
    "WBplots[1, 1].set_title(\"2nd layer bias\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weights\n",
    "a = tmp_mdl.fc1.weight.cpu().detach()\n",
    "b = tmp_mdl.fc2.weight.cpu().detach()\n",
    "#a = model_list[2].fc1.weight.cpu().detach()\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (torch.rand(4,4) - 0.5)/5\n",
    "print(c.max(),c.min())\n",
    "#print(a.max(),a.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(dg_model.toJSON()))\n",
    "print(dg_model, dg_model.fc1.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_model.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dg_model(ins[1]),\"\\n\",outs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.Tensor.uniform_(0.,1.)\n",
    "tmax=test_input[0][torch.argmax(test_input[0])]\n",
    "print(test_input, test_input.shape,tmax)\n",
    "test_input = test_input/tmax\n",
    "print(test_input)\n",
    "test_input = test_input.to('cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
